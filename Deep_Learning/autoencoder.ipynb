{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-07-01 00:00:00</th>\n",
       "      <td>10844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-07-01 00:30:00</th>\n",
       "      <td>8127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-07-01 01:00:00</th>\n",
       "      <td>6210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-07-01 01:30:00</th>\n",
       "      <td>4656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-07-01 02:00:00</th>\n",
       "      <td>3820</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     value\n",
       "timestamp                 \n",
       "2014-07-01 00:00:00  10844\n",
       "2014-07-01 00:30:00   8127\n",
       "2014-07-01 01:00:00   6210\n",
       "2014-07-01 01:30:00   4656\n",
       "2014-07-01 02:00:00   3820"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = '../data/default_nyc_taxi.csv'\n",
    "data = pd.read_csv(url, parse_dates=['timestamp'], index_col='timestamp')\n",
    "data = data[[\"value\"]]\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2014-07-01 00:00:00', '2014-07-01 00:30:00',\n",
       "               '2014-07-01 01:00:00', '2014-07-01 01:30:00',\n",
       "               '2014-07-01 02:00:00', '2014-07-01 02:30:00',\n",
       "               '2014-07-01 03:00:00', '2014-07-01 03:30:00',\n",
       "               '2014-07-01 04:00:00', '2014-07-01 04:30:00',\n",
       "               ...\n",
       "               '2015-01-31 19:00:00', '2015-01-31 19:30:00',\n",
       "               '2015-01-31 20:00:00', '2015-01-31 20:30:00',\n",
       "               '2015-01-31 21:00:00', '2015-01-31 21:30:00',\n",
       "               '2015-01-31 22:00:00', '2015-01-31 22:30:00',\n",
       "               '2015-01-31 23:00:00', '2015-01-31 23:30:00'],\n",
       "              dtype='datetime64[ns]', name='timestamp', length=10320, freq=None)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "period = 24 * 2\n",
    "data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From  2014-07-01 00:00:00  to  2015-01-31 23:30:00\n"
     ]
    }
   ],
   "source": [
    "print('From  ' + str(np.min(data.index)) + '  to  ' +str(np.max(data.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: 10320 \n",
      "Number of data per day: 48 \n",
      "Number of days: 215\n"
     ]
    }
   ],
   "source": [
    "print('Data size: %d \\nNumber of data per day: %d \\nNumber of days: %d' %(data.shape[0], period, data.shape[0] / period))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing value:  0\n"
     ]
    }
   ],
   "source": [
    "print('Missing value: ', data.isnull().to_numpy().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data into days - function\n",
    "def create_dataset(X, dates, period=1):\n",
    "    Xs = []\n",
    "    indexes = []\n",
    "    for i in range(int(len(X) / period)):\n",
    "        v = X.iloc[i*period: (i + 1)*period].values\n",
    "        indexes.append(dates[period*i])\n",
    "        Xs.append(v)        \n",
    "    return np.array(Xs), np.array(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape:  (215, 48)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-07-01</th>\n",
       "      <td>10844</td>\n",
       "      <td>8127</td>\n",
       "      <td>6210</td>\n",
       "      <td>4656</td>\n",
       "      <td>3820</td>\n",
       "      <td>2873</td>\n",
       "      <td>2369</td>\n",
       "      <td>2064</td>\n",
       "      <td>2221</td>\n",
       "      <td>2158</td>\n",
       "      <td>2515</td>\n",
       "      <td>4364</td>\n",
       "      <td>6526</td>\n",
       "      <td>11039</td>\n",
       "      <td>13857</td>\n",
       "      <td>15865</td>\n",
       "      <td>17920</td>\n",
       "      <td>20346</td>\n",
       "      <td>19539</td>\n",
       "      <td>20107</td>\n",
       "      <td>18984</td>\n",
       "      <td>17720</td>\n",
       "      <td>17249</td>\n",
       "      <td>18463</td>\n",
       "      <td>18908</td>\n",
       "      <td>18886</td>\n",
       "      <td>18178</td>\n",
       "      <td>19459</td>\n",
       "      <td>19546</td>\n",
       "      <td>20591</td>\n",
       "      <td>19380</td>\n",
       "      <td>18544</td>\n",
       "      <td>16228</td>\n",
       "      <td>15013</td>\n",
       "      <td>17203</td>\n",
       "      <td>19525</td>\n",
       "      <td>22966</td>\n",
       "      <td>27598</td>\n",
       "      <td>26827</td>\n",
       "      <td>24904</td>\n",
       "      <td>22875</td>\n",
       "      <td>20394</td>\n",
       "      <td>23401</td>\n",
       "      <td>24439</td>\n",
       "      <td>23318</td>\n",
       "      <td>21733</td>\n",
       "      <td>20104</td>\n",
       "      <td>16111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-07-02</th>\n",
       "      <td>13370</td>\n",
       "      <td>9945</td>\n",
       "      <td>7571</td>\n",
       "      <td>5917</td>\n",
       "      <td>4820</td>\n",
       "      <td>3634</td>\n",
       "      <td>2993</td>\n",
       "      <td>2535</td>\n",
       "      <td>2570</td>\n",
       "      <td>2485</td>\n",
       "      <td>2868</td>\n",
       "      <td>4482</td>\n",
       "      <td>6788</td>\n",
       "      <td>11078</td>\n",
       "      <td>13729</td>\n",
       "      <td>16700</td>\n",
       "      <td>19156</td>\n",
       "      <td>19953</td>\n",
       "      <td>19502</td>\n",
       "      <td>18994</td>\n",
       "      <td>17311</td>\n",
       "      <td>17904</td>\n",
       "      <td>17133</td>\n",
       "      <td>18589</td>\n",
       "      <td>19134</td>\n",
       "      <td>19259</td>\n",
       "      <td>18667</td>\n",
       "      <td>19078</td>\n",
       "      <td>18546</td>\n",
       "      <td>18593</td>\n",
       "      <td>17967</td>\n",
       "      <td>16624</td>\n",
       "      <td>14634</td>\n",
       "      <td>13888</td>\n",
       "      <td>17430</td>\n",
       "      <td>21919</td>\n",
       "      <td>23633</td>\n",
       "      <td>24512</td>\n",
       "      <td>24887</td>\n",
       "      <td>26872</td>\n",
       "      <td>22009</td>\n",
       "      <td>18259</td>\n",
       "      <td>20844</td>\n",
       "      <td>22576</td>\n",
       "      <td>22401</td>\n",
       "      <td>19056</td>\n",
       "      <td>17518</td>\n",
       "      <td>15307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-07-03</th>\n",
       "      <td>12646</td>\n",
       "      <td>10562</td>\n",
       "      <td>8416</td>\n",
       "      <td>7098</td>\n",
       "      <td>5826</td>\n",
       "      <td>4383</td>\n",
       "      <td>3270</td>\n",
       "      <td>2948</td>\n",
       "      <td>3146</td>\n",
       "      <td>3077</td>\n",
       "      <td>3000</td>\n",
       "      <td>4592</td>\n",
       "      <td>6486</td>\n",
       "      <td>10113</td>\n",
       "      <td>12240</td>\n",
       "      <td>14574</td>\n",
       "      <td>16778</td>\n",
       "      <td>18910</td>\n",
       "      <td>18350</td>\n",
       "      <td>17218</td>\n",
       "      <td>16097</td>\n",
       "      <td>16409</td>\n",
       "      <td>15893</td>\n",
       "      <td>16778</td>\n",
       "      <td>17604</td>\n",
       "      <td>18665</td>\n",
       "      <td>19045</td>\n",
       "      <td>19261</td>\n",
       "      <td>19363</td>\n",
       "      <td>19078</td>\n",
       "      <td>18193</td>\n",
       "      <td>16635</td>\n",
       "      <td>14615</td>\n",
       "      <td>13759</td>\n",
       "      <td>17008</td>\n",
       "      <td>19595</td>\n",
       "      <td>21328</td>\n",
       "      <td>22661</td>\n",
       "      <td>29985</td>\n",
       "      <td>21501</td>\n",
       "      <td>22684</td>\n",
       "      <td>22188</td>\n",
       "      <td>22663</td>\n",
       "      <td>19573</td>\n",
       "      <td>17136</td>\n",
       "      <td>16606</td>\n",
       "      <td>16166</td>\n",
       "      <td>16020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-07-04</th>\n",
       "      <td>15591</td>\n",
       "      <td>14395</td>\n",
       "      <td>12535</td>\n",
       "      <td>11341</td>\n",
       "      <td>9980</td>\n",
       "      <td>8404</td>\n",
       "      <td>7200</td>\n",
       "      <td>6578</td>\n",
       "      <td>5657</td>\n",
       "      <td>4474</td>\n",
       "      <td>3459</td>\n",
       "      <td>3276</td>\n",
       "      <td>3595</td>\n",
       "      <td>4240</td>\n",
       "      <td>4828</td>\n",
       "      <td>4926</td>\n",
       "      <td>5165</td>\n",
       "      <td>5776</td>\n",
       "      <td>7338</td>\n",
       "      <td>7839</td>\n",
       "      <td>8623</td>\n",
       "      <td>9731</td>\n",
       "      <td>11024</td>\n",
       "      <td>13231</td>\n",
       "      <td>13613</td>\n",
       "      <td>13737</td>\n",
       "      <td>15574</td>\n",
       "      <td>14226</td>\n",
       "      <td>18480</td>\n",
       "      <td>18265</td>\n",
       "      <td>16575</td>\n",
       "      <td>16417</td>\n",
       "      <td>14703</td>\n",
       "      <td>13469</td>\n",
       "      <td>12105</td>\n",
       "      <td>11676</td>\n",
       "      <td>15487</td>\n",
       "      <td>15077</td>\n",
       "      <td>14999</td>\n",
       "      <td>14487</td>\n",
       "      <td>14415</td>\n",
       "      <td>13796</td>\n",
       "      <td>14036</td>\n",
       "      <td>14021</td>\n",
       "      <td>15593</td>\n",
       "      <td>16589</td>\n",
       "      <td>17984</td>\n",
       "      <td>18035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-07-05</th>\n",
       "      <td>17576</td>\n",
       "      <td>16189</td>\n",
       "      <td>14441</td>\n",
       "      <td>12535</td>\n",
       "      <td>11006</td>\n",
       "      <td>9151</td>\n",
       "      <td>8010</td>\n",
       "      <td>7096</td>\n",
       "      <td>6407</td>\n",
       "      <td>4421</td>\n",
       "      <td>3126</td>\n",
       "      <td>2514</td>\n",
       "      <td>2550</td>\n",
       "      <td>3148</td>\n",
       "      <td>3658</td>\n",
       "      <td>4345</td>\n",
       "      <td>4682</td>\n",
       "      <td>6248</td>\n",
       "      <td>7454</td>\n",
       "      <td>9010</td>\n",
       "      <td>10280</td>\n",
       "      <td>11488</td>\n",
       "      <td>11595</td>\n",
       "      <td>13098</td>\n",
       "      <td>12623</td>\n",
       "      <td>13031</td>\n",
       "      <td>13263</td>\n",
       "      <td>13349</td>\n",
       "      <td>13822</td>\n",
       "      <td>13716</td>\n",
       "      <td>13919</td>\n",
       "      <td>14203</td>\n",
       "      <td>13179</td>\n",
       "      <td>13708</td>\n",
       "      <td>13897</td>\n",
       "      <td>14740</td>\n",
       "      <td>14575</td>\n",
       "      <td>16085</td>\n",
       "      <td>18182</td>\n",
       "      <td>16861</td>\n",
       "      <td>14140</td>\n",
       "      <td>14477</td>\n",
       "      <td>15293</td>\n",
       "      <td>15457</td>\n",
       "      <td>16048</td>\n",
       "      <td>17477</td>\n",
       "      <td>16391</td>\n",
       "      <td>17006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0      1      2      3   ...     44     45     46     47\n",
       "2014-07-01  10844   8127   6210   4656  ...  23318  21733  20104  16111\n",
       "2014-07-02  13370   9945   7571   5917  ...  22401  19056  17518  15307\n",
       "2014-07-03  12646  10562   8416   7098  ...  17136  16606  16166  16020\n",
       "2014-07-04  15591  14395  12535  11341  ...  15593  16589  17984  18035\n",
       "2014-07-05  17576  16189  14441  12535  ...  16048  17477  16391  17006\n",
       "\n",
       "[5 rows x 48 columns]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new data frame\n",
    "df, dates = create_dataset(data.value, data.index, period)\n",
    "df = pd.DataFrame(df, dates)\n",
    "\n",
    "print('df.shape: ', df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  0.55\n",
      "\n",
      "\n",
      "TRAIN SET:  from  2014-07-01  to  2014-10-26\n",
      "Data size:  118\n",
      "Number of days:  2\n",
      "\n",
      "\n",
      "TEST SET:  from  2014-10-27  to  2015-01-31\n",
      "Data size:  97\n",
      "Number of days:  2\n"
     ]
    }
   ],
   "source": [
    "# splitting\n",
    "ratio = 0.55\n",
    "train_size = int(df.shape[0] * ratio)\n",
    "X_train = df[:train_size]\n",
    "X_test = df[train_size:]\n",
    "\n",
    "dates_train = np.array(df.index[:train_size], dtype='datetime64[D]')\n",
    "dates_test = np.array(df.index[train_size:], dtype='datetime64[D]')\n",
    "\n",
    "\n",
    "# info\n",
    "print('Train size: ', ratio)\n",
    "print('\\n\\nTRAIN SET:  from  ' + str(np.min(dates_train)) + '  to  ' +str(np.max(dates_train)))\n",
    "print('Data size: ', X_train.shape[0])\n",
    "print('Number of days: ', int(X_train.shape[0] / period))\n",
    "print('\\n\\nTEST SET:  from  ' + str(np.min(dates_test)) + '  to  ' +str(np.max(dates_test)))\n",
    "print('Data size: ', X_test.shape[0])\n",
    "print('Number of days: ', int(X_test.shape[0] / period))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "dim_hidden1 = 32\n",
    "dim_hidden2 = 16\n",
    "dim_hidden3 = 8\n",
    "\n",
    "\n",
    "# model\n",
    "class Autoencoder(Model):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = Sequential([\n",
    "          Dense(dim_hidden1, activation=\"relu\"),\n",
    "          Dense(dim_hidden2, activation=\"relu\"),\n",
    "          Dense(dim_hidden3, activation=\"relu\")])\n",
    "\n",
    "        self.decoder = Sequential([\n",
    "          Dense(dim_hidden2, activation=\"relu\"),\n",
    "          Dense(dim_hidden1, activation=\"relu\"),\n",
    "          Dense(period, activation=\"sigmoid\")])\n",
    "\n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "epochs = 100\n",
    "batch_size = 20\n",
    "validation_split = 0.1\n",
    "shuffle = False\n",
    "\n",
    "\n",
    "# fitting model\n",
    "autoencoder = Autoencoder()\n",
    "autoencoder.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 1s 22ms/step - loss: 1.2735 - val_loss: 1.0276\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.2567 - val_loss: 1.0178\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.2378 - val_loss: 1.0062\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.2128 - val_loss: 0.9919\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.1786 - val_loss: 0.9753\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.1339 - val_loss: 0.9577\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0804 - val_loss: 0.9401\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0215 - val_loss: 0.9223\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9615 - val_loss: 0.9047\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9051 - val_loss: 0.8885\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.8570 - val_loss: 0.8708\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.8177 - val_loss: 0.8471\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7865 - val_loss: 0.8199\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7637 - val_loss: 0.7931\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7477 - val_loss: 0.7673\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7354 - val_loss: 0.7426\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7254 - val_loss: 0.7192\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7170 - val_loss: 0.6964\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7097 - val_loss: 0.6739\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.7033 - val_loss: 0.6525\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6977 - val_loss: 0.6327\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6927 - val_loss: 0.6141\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6882 - val_loss: 0.5956\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6839 - val_loss: 0.5768\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6799 - val_loss: 0.5592\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6765 - val_loss: 0.5440\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6735 - val_loss: 0.5308\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6707 - val_loss: 0.5188\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6681 - val_loss: 0.5087\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6662 - val_loss: 0.5022\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6649 - val_loss: 0.4980\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.6636 - val_loss: 0.4945\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6622 - val_loss: 0.4914\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6611 - val_loss: 0.4887\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6601 - val_loss: 0.4862\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.6593 - val_loss: 0.4841\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6585 - val_loss: 0.4824\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6579 - val_loss: 0.4810\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6573 - val_loss: 0.4800\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6567 - val_loss: 0.4790\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6560 - val_loss: 0.4783\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6554 - val_loss: 0.4776\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.6548 - val_loss: 0.4769\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6542 - val_loss: 0.4763\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6537 - val_loss: 0.4759\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6532 - val_loss: 0.4757\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6528 - val_loss: 0.4757\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6523 - val_loss: 0.4756\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6519 - val_loss: 0.4756\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6516 - val_loss: 0.4754\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6512 - val_loss: 0.4753\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6509 - val_loss: 0.4751\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6506 - val_loss: 0.4749\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6503 - val_loss: 0.4747\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.6500 - val_loss: 0.4746\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6497 - val_loss: 0.4745\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6494 - val_loss: 0.4744\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6491 - val_loss: 0.4742\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6488 - val_loss: 0.4741\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6485 - val_loss: 0.4739\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.6482 - val_loss: 0.4737\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6479 - val_loss: 0.4735\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6475 - val_loss: 0.4733\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6472 - val_loss: 0.4728\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6468 - val_loss: 0.4721\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6464 - val_loss: 0.4716\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6461 - val_loss: 0.4705\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.6458 - val_loss: 0.4697\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.6454 - val_loss: 0.4695\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6450 - val_loss: 0.4690\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.6447 - val_loss: 0.4687\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.6443 - val_loss: 0.4688\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6439 - val_loss: 0.4684\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6435 - val_loss: 0.4683\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6431 - val_loss: 0.4680\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6427 - val_loss: 0.4680\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6423 - val_loss: 0.4675\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6418 - val_loss: 0.4675\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6414 - val_loss: 0.4673\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.6409 - val_loss: 0.4672\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.6404 - val_loss: 0.4674\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6400 - val_loss: 0.4671\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.6395 - val_loss: 0.4669\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6390 - val_loss: 0.4666\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6385 - val_loss: 0.4665\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6381 - val_loss: 0.4665\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.6376 - val_loss: 0.4664\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.6372 - val_loss: 0.4662\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6368 - val_loss: 0.4662\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6364 - val_loss: 0.4654\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.6359 - val_loss: 0.4652\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 0s 10ms/step - loss: 0.6356 - val_loss: 0.4652\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6352 - val_loss: 0.4648\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6348 - val_loss: 0.4645\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6344 - val_loss: 0.4646\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.6341 - val_loss: 0.4643\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6338 - val_loss: 0.4642\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.6334 - val_loss: 0.4638\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6331 - val_loss: 0.4634\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.6328 - val_loss: 0.4635\n"
     ]
    }
   ],
   "source": [
    "history = autoencoder.fit(X_train, X_train, \n",
    "                          epochs = epochs, \n",
    "                          batch_size = batch_size, \n",
    "                          validation_split = validation_split, \n",
    "                          shuffle = shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data = autoencoder.encoder(X_train).numpy()\n",
    "decoded_data = autoencoder.decoder(encoded_data).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data = autoencoder.encoder(X_test).numpy()\n",
    "decoded_data = autoencoder.decoder(encoded_data).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 998us/step\n"
     ]
    }
   ],
   "source": [
    "reconstruction = autoencoder.predict(X_train)\n",
    "loss_train = tf.keras.losses.mae(reconstruction, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "reconstruction_test = autoencoder.predict(X_test)\n",
    "loss_test = tf.keras.losses.mae(reconstruction_test, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1298712800324364\n"
     ]
    }
   ],
   "source": [
    "threshold = np.mean(loss_train) + 1.75*np.std(loss_train)\n",
    "print(threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({'date': np.array(dates_test, dtype='datetime64[D]'), \n",
    "                        'loss': loss_test})\n",
    "\n",
    "results = results.set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_loss = (loss_test - np.min(loss_test)) / (np.max(loss_test) - np.min(loss_test)) * 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = loss_test.numpy() >= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2014-11-01' '2014-11-27' '2014-12-07' '2014-12-14' '2014-12-24'\n",
      " '2014-12-25' '2014-12-26' '2014-12-27' '2014-12-28' '2015-01-01'\n",
      " '2015-01-04' '2015-01-11' '2015-01-18' '2015-01-25' '2015-01-26'\n",
      " '2015-01-27']\n"
     ]
    }
   ],
   "source": [
    "ref = np.array(dates_test[y_pred])\n",
    "\n",
    "ref = pd.to_datetime(ref, format='%Y/%m/%d')\n",
    "\n",
    "print(np.array(dates_test[y_pred]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_data = data.reset_index().copy()\n",
    "\n",
    "autoencoder_data['date'] = pd.to_datetime(autoencoder_data['timestamp']).dt.date\n",
    "\n",
    "# Modifier les valeurs dans 'pred' où la date est dans 'ref'\n",
    "autoencoder_data[\"pred\"] = 0 \n",
    "autoencoder_data.loc[autoencoder_data['date'].isin(ref.date), 'pred'] = 1\n",
    "\n",
    "# Supprimer la colonne 'date' si elle n'est plus nécessaire\n",
    "autoencoder_data = autoencoder_data.drop('date', axis=1)\n",
    "\n",
    "autoencoder_data.to_csv(\"../data/autoencoder_nyc_taxi.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = pd.read_csv(\"../data/total_nyc_taxi.csv\",index_col=0)\n",
    "total[\"autoencoder_pred\"] = autoencoder_data[\"pred\"]\n",
    "\n",
    "total.to_csv(\"../data/total_nyc_taxi.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
